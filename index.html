<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Manan Suri</title> <meta name="author" content="Manan Suri"> <meta name="description" content="Computer Engineering Senior, Interested in AI, Data, and Software "> <meta name="keywords" content="ai, data science, software"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%91%BE&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://manansuri.com/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories</a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv</a> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Manan</span> Suri </h1> <p class="desc">MS in Computer Science at University of Maryland, College Park</p> </header> <article> <div class="profile float-right"> <figure> <picture> <source class="responsive-img-srcset" media="(max-width: 480px)" srcset="/assets/img/manansuri-480.webp"></source> <source class="responsive-img-srcset" media="(max-width: 800px)" srcset="/assets/img/manansuri-800.webp"></source> <source class="responsive-img-srcset" media="(max-width: 1400px)" srcset="/assets/img/manansuri-1400.webp"></source> <img src="/assets/img/manansuri.jpeg?2f6fdb2d86b3f7d40a4a10d009afe8ac" class="img-fluid z-depth-1 rounded" width="auto" height="auto" alt="manansuri.jpeg" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <div class="address"> </div> </div> <div class="clearfix"> <p>I am an MS in Computer Science candidate at University of Maryland, College Park. I completed my B.Tech. in Computer Enginering from Netaji Subhas University of Technology, New Delhi in 2024.</p> <p>I served as a <code class="language-plaintext highlighter-rouge">Data Science for Social Good Fellow</code> at the <code class="language-plaintext highlighter-rouge">University of Warwick</code> in 2023, collaborating with the <a href="https://ati.io/" rel="external nofollow noopener" target="_blank">Algorithmic Transparency Institute</a> on greenwashing and greenmessaging with <a href="https://people.miami.edu/profile/19dbd62ef7e2e5fcd155ef0c53f35bc8" rel="external nofollow noopener" target="_blank">Dr. Geoffrey Supran</a> and <a href="https://findanexpert.unimelb.edu.au/profile/1028119-john-cook" rel="external nofollow noopener" target="_blank">Dr. John Cook</a>.</p> <p>At <a href="https://www.scalenut.com/" rel="external nofollow noopener" target="_blank">Scalenut</a>, I worked on document ranking, retrieval functions, and developed a fact annotator.</p> <p>At <code class="language-plaintext highlighter-rouge">Northeastern University</code>, I managed research teams and conducted research on Depressive Tendencies, Trigger Warnings, and Fine-grained Sexism Detection with <a href="https://www.khoury.northeastern.edu/people/divya-chaudhary/" rel="external nofollow noopener" target="_blank">Dr. Divya Chaudhary</a> and <a href="https://www.khoury.northeastern.edu/people/ian-gorton/" rel="external nofollow noopener" target="_blank">Dr. Ian Gorton</a>.</p> <p>I am currently working under <a href="https://www.cs.umd.edu/people/dmanocha" rel="external nofollow noopener" target="_blank">Dr. Dinesh Manocha</a> at the <code class="language-plaintext highlighter-rouge">GAMMA Lab, University of Maryland, College Park</code>, on multimodal document based problems.</p> </div> <h2><a href="/news/" style="color: inherit;">news</a></h2> <div class="news"> <div class="table-responsive" style="max-height: 60vw"> <table class="table table-sm table-borderless"> <tr> <th scope="row">Jan 22, 2025</th> <td> Our paper “VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation” got accepted at NAACL 2025, main conference! </td> </tr> <tr> <th scope="row">Nov 20, 2024</th> <td> I presented our paper “DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding” at EMNLP 2024, in Miami! I was also a Volunteer Coordinator at the conference. </td> </tr> <tr> <th scope="row">Nov 1, 2024</th> <td> Served as a reviewer for ARR October 2024 Cycle (NAACL), and ICASSP 2025. </td> </tr> <tr> <th scope="row">Sep 20, 2024</th> <td> Our paper “DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding” got accepted at EMNLP 2024, main conference! </td> </tr> <tr> <th scope="row">Aug 26, 2024</th> <td> I have begun my MS in Computer Science at the University of Maryland, College Park. </td> </tr> </table> </div> </div> <h2><a href="/publications/" style="color: inherit;">selected publications</a></h2> <div class="publications"> <h2 class="bibliography">2024</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="suri2024doccommand" class="col-sm-8"> <a href="https://openreview.net/forum?id=inQ9bW5AQz" rel="external nofollow noopener" target="_blank"><div class="title">Doc2Command: Furthering Language Guided Document Editing</div></a> <div class="author"> <em>Manan Suri</em>, Puneet Mathur, Ramit Sawhney, Preslav Nakov, and <a href="https://www.cs.umd.edu/people/dmanocha" rel="external nofollow noopener" target="_blank">Dinesh Manocha</a> </div> <div class="periodical"> <em>In The Second Tiny Papers Track at ICLR 2024</em>, 2024 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="suri-etal-2024-docedit" class="col-sm-8"> <a href="https://aclanthology.org/2024.emnlp-main.867" rel="external nofollow noopener" target="_blank"><div class="title">DocEdit-v2: Document Structure Editing Via Multimodal LLM Grounding</div></a> <div class="author"> <em>Manan Suri</em>, Puneet Mathur, Franck Dernoncourt, Rajiv Jain, Vlad I Morariu, Ramit Sawhney, Preslav Nakov, and <a href="https://www.cs.umd.edu/people/dmanocha" rel="external nofollow noopener" target="_blank">Dinesh Manocha</a> </div> <div class="periodical"> <em>In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing</em>, Nov 2024 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Document structure editing involves manipulating localized textual, visual, and layout components in document images based on the user’s requests. Past works have shown that multimodal grounding of user requests in the document image and identifying the accurate structural components and their associated attributes remain key challenges for this task. To address these, we introduce the DocEditAgent, a novel framework that performs end-to-end document editing by leveraging Large Multimodal Models (LMMs). It consists of three novel components – (1) Doc2Command to simultaneously localize edit regions of interest (RoI) and disambiguate user edit requests into edit commands. (2) LLM-based Command Reformulation prompting to tailor edit commands originally intended for specialized software into edit instructions suitable for generalist LMMs. (3) Moreover, DocEditAgent processes these outputs via Large Multimodal Models like GPT-4V and Gemini, to parse the document layout, execute edits on grounded Region of Interest (RoI), and generate the edited document image. Extensive experiments on the DocEdit dataset show that DocEditAgent significantly outperforms strong baselines on edit command generation (2-33%), RoI bounding box detection (12-31%), and overall document editing (1-12%) tasks.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2023</h2> <ol class="bibliography"> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ghosh-etal-2023-aclm" class="col-sm-8"> <a href="https://aclanthology.org/2023.acl-long.8" rel="external nofollow noopener" target="_blank"><div class="title">ACLM: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex NER</div></a> <div class="author"> Sreyan Ghosh, Utkarsh Tyagi, <em>Manan Suri</em>, Sonal Kumar, Ramaneswaran S, and <a href="https://www.cs.umd.edu/people/dmanocha" rel="external nofollow noopener" target="_blank">Dinesh Manocha</a> </div> <div class="periodical"> <em>In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)</em>, Jul 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a class="bibtex btn btn-sm z-depth-0" role="button">Bib</a> <a href="https://aclanthology.org/2023.acl-long.8" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">HTML</a> <a href="https://aclanthology.org/2023.acl-long.8.pdf" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">PDF</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>Complex Named Entity Recognition (NER) is the task of detecting linguistically complex named entities in low-context text. In this paper, we present ACLM Attention-map aware keyword selection for Conditional Language Model fine-tuning), a novel data augmentation approach based on conditional generation, to address the data scarcity problem in low-resource complex NER. ACLM alleviates the context-entity mismatch issue, a problem existing NER data augmentation techniques suffer from and often generates incoherent augmentations by placing complex named entities in the wrong context. ACLM builds on BART and is optimized on a novel text reconstruction or denoising task - we use selective masking (aided by attention maps) to retain the named entities and certain keywords in the input sentence that provide contextually relevant additional knowledge or hints about the named entities. Compared with other data augmentation strategies, ACLM can generate more diverse and coherent augmentations preserving the true word sense of complex entities in the sentence. We demonstrate the effectiveness of ACLM both qualitatively and quantitatively on monolingual, cross-lingual, and multilingual complex NER across various low-resource settings. ACLM outperforms all our neural baselines by a significant margin (1%-36%). In addition, we demonstrate the application of ACLM to other domains that suffer from data scarcity (e.g., biomedical). In practice, ACLM generates more effective and factual augmentations for these domains than prior methods.</p> </div> <div class="bibtex hidden"> <figure class="highlight"><pre><code class="language-bibtex" data-lang="bibtex"><span class="nc">@inproceedings</span><span class="p">{</span><span class="nl">ghosh-etal-2023-aclm</span><span class="p">,</span>
  <span class="na">title</span> <span class="p">=</span> <span class="s">{{ACLM}: A Selective-Denoising based Generative Data Augmentation Approach for Low-Resource Complex {NER}}</span><span class="p">,</span>
  <span class="na">author</span> <span class="p">=</span> <span class="s">{Ghosh, Sreyan and Tyagi, Utkarsh and Suri, Manan and Kumar, Sonal and S, Ramaneswaran and Manocha, Dinesh}</span><span class="p">,</span>
  <span class="na">booktitle</span> <span class="p">=</span> <span class="s">{Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)}</span><span class="p">,</span>
  <span class="na">month</span> <span class="p">=</span> <span class="nv">jul</span><span class="p">,</span>
  <span class="na">year</span> <span class="p">=</span> <span class="s">{2023}</span><span class="p">,</span>
  <span class="na">address</span> <span class="p">=</span> <span class="s">{Toronto, Canada}</span><span class="p">,</span>
  <span class="na">publisher</span> <span class="p">=</span> <span class="s">{Association for Computational Linguistics}</span><span class="p">,</span>
  <span class="na">url</span> <span class="p">=</span> <span class="s">{https://aclanthology.org/2023.acl-long.8}</span><span class="p">,</span>
  <span class="na">doi</span> <span class="p">=</span> <span class="s">{10.18653/v1/2023.acl-long.8}</span><span class="p">,</span>
  <span class="na">pages</span> <span class="p">=</span> <span class="s">{104--125}</span><span class="p">,</span>
<span class="p">}</span></code></pre></figure> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="ghosh2023cosyn" class="col-sm-8"> <a href="https://aclanthology.org/2023.emnlp-main.377/" rel="external nofollow noopener" target="_blank"><div class="title">CoSyn: Detecting Implicit Hate Speech in Online Conversations Using a Context Synergized Hyperbolic Network</div></a> <div class="author"> Sreyan Ghosh, <em>Manan Suri</em>, Purva Chiniya, Utkarsh Tyagi, Sonal Kumar, and <a href="https://www.cs.umd.edu/people/dmanocha" rel="external nofollow noopener" target="_blank">Dinesh Manocha</a> </div> <div class="periodical"> <em>In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> </div> <div class="badges"> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="10.1145/3578741.3578817" class="col-sm-8"> <a href="https://doi.org/10.1145/3578741.3578817" rel="external nofollow noopener" target="_blank"><div class="title">I Don’t Feel so Good! Detecting Depressive Tendencies Using Transformer-Based Multimodal Frameworks</div></a> <div class="author"> <em>Manan Suri</em>, Nalin Semwal, <a href="https://www.khoury.northeastern.edu/people/divya-chaudhary/" rel="external nofollow noopener" target="_blank">Divya Chaudhary</a>, <a href="https://www.khoury.northeastern.edu/people/ian-gorton/" rel="external nofollow noopener" target="_blank">Ian Gorton</a>, and <a href="http://nsut.ac.in/en/node/242" rel="external nofollow noopener" target="_blank">Bijendra Kumar</a> </div> <div class="periodical"> <em>In Proceedings of the 2022 5th International Conference on Machine Learning and Natural Language Processing</em>, Dec 2023 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>One of the most common mental illnesses that affects 5% of adults globally is depression. The advancement of social media has meant that more and more people have gained a platform to voice their thoughts and beliefs. People’s social media interactions and posted content can be used to infer critical characteristics such as depressive tendencies which will allow for timely intervention and help. This paper describes a novel supervised approach to detect depressive tendencies in Twitter users using multimodal frameworks which account for user interaction and online behaviour in addition to the tweet content processed using transformers like BERT. The performance of three multimodal frameworks is described with different methods for combining modalities. The best result is obtained a cross-modality based model which improves the baseline by 12% points.</p> </div> </div> </div> </li> </ol> <h2 class="bibliography">2022</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"></div> <div id="suri-2022-pickle" class="col-sm-8"> <a href="https://aclanthology.org/2022.semeval-1.63" rel="external nofollow noopener" target="_blank"><div class="title">Boosting Pre-trained Language Models with Task Specific Metadata and Cost Sensitive Learning</div></a> <div class="author"> <em>Manan Suri</em> </div> <div class="periodical"> <em>In Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022)</em>, Jul 2022 </div> <div class="periodical"> </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> </div> <div class="badges"> </div> <div class="abstract hidden"> <p>This paper describes our system for Task 4 of SemEval 2022: Patronizing and Condescending Language Detection. Patronizing and Condescending Language (PCL) refers to language used with respect to vulnerable communities that portrays them in a pitiful way and is reflective of a sense of superiority. Task 4 involved binary classification (Subtask 1) and multi-label classification (Subtask 2) of Patronizing and Condescending Language (PCL). For our system, we experimented with fine-tuning different transformer-based pre-trained models including BERT, DistilBERT, RoBERTa and ALBERT. Further, we have used token separated metadata in order to improve our model by helping it contextualize different communities with respect to PCL. We faced the challenge of class imbalance, which we solved by experimenting with different class weighting schemes. Our models were effective in both subtasks, with the best performance coming out of models with Effective Number of Samples (ENS) class weighting and token separated metadata in both subtasks. For subtask 1 and subtask 2, our best models were finetuned BERT and RoBERTa models respectively.</p> </div> </div> </div> </li></ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%6D%61%6E%61%6E%73%75%72%69%32%37@%67%6D%61%69%6C.%63%6F%6D" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=KbVluf4AAAAJ" title="Google Scholar" rel="external nofollow noopener" target="_blank"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/MananSuri27" title="GitHub" rel="external nofollow noopener" target="_blank"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/manansuri27" title="LinkedIn" rel="external nofollow noopener" target="_blank"><i class="fab fa-linkedin"></i></a> <a href="https://medium.com/@manansuri" title="Medium" rel="external nofollow noopener" target="_blank"><i class="fab fa-medium"></i></a> </div> <div class="contact-note"> Drop me an email, or say hi on LinkedIn! </div> </div> </article> </div> </div> <footer class="sticky-bottom mt-5"> <div class="container"> © Copyright 2025 Manan Suri. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script async src="https://www.googletagmanager.com/gtag/js?id=G-YYF22TSHKM"></script> <script>function gtag(){window.dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-YYF22TSHKM");</script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>